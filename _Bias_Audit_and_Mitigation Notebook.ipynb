{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1tbuOJ0uJCE"
      },
      "source": [
        "# **Bias Audit Report: Adult Income Dataset**\n",
        "\n",
        "\n",
        "### **1. Scenario**\n",
        "This notebook conducts a thorough bias audit of a machine learning model trained on the **UCI Adult Income Dataset** (also known as the \"Census Income\" dataset). The goal is to predict whether an individual's income exceeds $50K/year. We investigate potential **gender bias** (sex: Male vs. Female) in the model's predictions, quantify the bias using standard fairness metrics, apply mitigation techniques, and evaluate their effectiveness.\n",
        "\n",
        "**Libraries Used:**\n",
        "- `aif360` (IBM AI Fairness 360 Toolkit) for metrics and mitigation\n",
        "- `fairlearn` for additional metrics\n",
        "- `scikit-learn` for model building and preprocessing\n",
        "- `pandas`, `numpy` for data manipulation\n",
        "- `matplotlib`, `seaborn` for visualizations\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SekkrF6yuSsz"
      },
      "outputs": [],
      "source": [
        "# Install necessary fairness toolkits\n",
        "!pip install aif360 fairlearn\n",
        "\n",
        "# Import standard libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Import AIF360 components\n",
        "from aif360.datasets import BinaryLabelDataset, AdultDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "from aif360.algorithms.postprocessing import RejectOptionClassification\n",
        "\n",
        "# Import Fairlearn for alternative metrics\n",
        "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp4ENdu0ua6b"
      },
      "source": [
        "### **2. Load and Preprocess the Dataset**\n",
        "We load the dataset directly from the AIF360 package. The protected attribute we are analyzing is **'sex'**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WTFQISxucQj"
      },
      "outputs": [],
      "source": [
        "# Load the dataset from AIF360\n",
        "# The modern way to specify the favorable label and protected attribute is via a custom transform function.\n",
        "# This approach is more robust across different versions of AIF360.\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "# Define the directory where AIF360 expects the data\n",
        "data_dir = '/usr/local/lib/python3.12/dist-packages/aif360/data/raw/adult'\n",
        "os.makedirs(data_dir, exist_ok=True) # Create the directory if it doesn't exist\n",
        "\n",
        "# Define the URLs for the dataset files\n",
        "adult_data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "adult_test_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
        "adult_names_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names'\n",
        "\n",
        "# Download the files if they don't exist\n",
        "for url in [adult_data_url, adult_test_url, adult_names_url]:\n",
        "    filename = os.path.join(data_dir, os.path.basename(url))\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"Downloading {url} to {filename}...\")\n",
        "        urllib.request.urlretrieve(url, filename)\n",
        "        print(\"Download complete.\")\n",
        "    else:\n",
        "        print(f\"{filename} already exists.\")\n",
        "\n",
        "\n",
        "# 1. First, define a function to transform the native dataset\n",
        "def custom_preprocessing(df):\n",
        "    \"\"\"Custom preprocessing for the Adult dataset.\"\"\"\n",
        "    # The AIF360 AdultDataset maps 'Male' to 1 and 'Female' to 0.\n",
        "    # We will keep this mapping for the 'sex' attribute.\n",
        "    # For the label 'income', it maps '>50K' to 1 and '<=50K' to 0.\n",
        "    # This function is often used for more complex filtering but we can use it to ensure correct setup.\n",
        "    return df\n",
        "\n",
        "# 2. Load the dataset using the more universal parameters\n",
        "# Load the training data\n",
        "adult_dataset_train = AdultDataset(protected_attribute_names=['sex'],\n",
        "                                   privileged_classes=[['Male']], # 'Male' is the privileged group (encoded as 1)\n",
        "                                   categorical_features=['workclass', 'education', 'marital-status',\n",
        "                                                         'occupation', 'relationship', 'race', 'native-country'],\n",
        "                                   features_to_drop=['fnlwgt'], # Often dropped as it is a weighting factor, not a feature\n",
        "                                   custom_preprocessing=custom_preprocessing,\n",
        "                                   na_values=['?'])\n",
        "\n",
        "# Load the test data (AIF360 automatically handles adult.test when present)\n",
        "adult_dataset_test = AdultDataset(protected_attribute_names=['sex'],\n",
        "                                  privileged_classes=[['Male']],\n",
        "                                  categorical_features=['workclass', 'education', 'marital-status',\n",
        "                                                        'occupation', 'relationship', 'race', 'native-country'],\n",
        "                                  features_to_drop=['fnlwgt'],\n",
        "                                  custom_preprocessing=custom_preprocessing,\n",
        "                                  na_values=['?']) # Removed test=True\n",
        "\n",
        "# Get the privileged and unprivileged groups from the dataset itself to avoid errors\n",
        "privileged_group = adult_dataset_train.privileged_protected_attributes\n",
        "unprivileged_group = adult_dataset_train.unprivileged_protected_attributes\n",
        "\n",
        "print(\"Privileged Group (Male):\", privileged_group)\n",
        "print(\"Unprivileged Group (Female):\", unprivileged_group)\n",
        "print(\"\\nLabel Names (automatically set by AIF360):\", adult_dataset_train.label_names)\n",
        "print(\"Favorable Label (>50K):\", adult_dataset_train.favorable_label)\n",
        "print(\"Unfavorable Label (<=50K):\", adult_dataset_train.unfavorable_label)\n",
        "\n",
        "# Convert to pandas DataFrames for easier exploration\n",
        "# Ensure the correct split happens if AIF360 loads both\n",
        "# AIF360's AdultDataset automatically splits into train/test if adult.data and adult.test are present.\n",
        "# We need to re-instantiate to make sure we get separate train/test objects for clarity and downstream use.\n",
        "\n",
        "adult_dataset_train = AdultDataset(protected_attribute_names=['sex'],\n",
        "                                   privileged_classes=[['Male']],\n",
        "                                   categorical_features=['workclass', 'education', 'marital-status',\n",
        "                                                         'occupation', 'relationship', 'race', 'native-country'],\n",
        "                                   features_to_drop=['fnlwgt'],\n",
        "                                   custom_preprocessing=custom_preprocessing,\n",
        "                                   na_values=['?']) # This loads the training data\n",
        "\n",
        "adult_dataset_test = AdultDataset(protected_attribute_names=['sex'],\n",
        "                                  privileged_classes=[['Male']],\n",
        "                                  categorical_features=['workclass', 'education', 'marital-status',\n",
        "                                                        'occupation', 'relationship', 'race', 'native-country'],\n",
        "                                  features_to_drop=['fnlwgt'],\n",
        "                                  custom_preprocessing=custom_preprocessing,\n",
        "                                  na_values=['?']) # This loads the test data\n",
        "\n",
        "# Convert to pandas DataFrames for easier exploration\n",
        "train_df = adult_dataset_train.convert_to_dataframe()[0]\n",
        "test_df = adult_dataset_test.convert_to_dataframe()[0]\n",
        "\n",
        "print(\"\\nTraining Data Shape:\", train_df.shape)\n",
        "print(\"Test Data Shape:\", test_df.shape)\n",
        "print(\"\\nFirst few rows of training data:\")\n",
        "display(train_df.head())\n",
        "print(\"\\nFirst few rows of test data:\")\n",
        "display(test_df.head())\n",
        "\n",
        "# Explicitly display dataframes to ensure they are in the notebook's state\n",
        "display(train_df)\n",
        "display(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krJs7uh5ugSb"
      },
      "outputs": [],
      "source": [
        "# Check the distribution of the target variable and the protected attribute\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Target variable distribution\n",
        "if 'train_df' in locals() or 'train_df' in globals():\n",
        "    train_df['income-per-year'].value_counts().plot(kind='bar', ax=ax[0], color=['skyblue', 'salmon'])\n",
        "    ax[0].set_title('Distribution of Target Variable (Income)')\n",
        "    ax[0].set_ylabel('Count')\n",
        "    ax[0].set_xticklabels(['<=50K', '>50K'], rotation=0)\n",
        "else:\n",
        "    ax[0].set_title('Training data not loaded yet')\n",
        "    ax[0].text(0.5, 0.5, 'train_df not found', horizontalalignment='center', verticalalignment='center', transform=ax[0].transAxes)\n",
        "\n",
        "\n",
        "# Protected attribute distribution\n",
        "if 'train_df' in locals() or 'train_df' in globals():\n",
        "    train_df['sex'].value_counts().plot(kind='bar', ax=ax[1], color=['lightgreen', 'plum'])\n",
        "    ax[1].set_title('Distribution of Protected Attribute (Sex)')\n",
        "    ax[1].set_ylabel('Count')\n",
        "    ax[1].set_xticklabels(['Female (0)', 'Male (1)'], rotation=0)\n",
        "else:\n",
        "    ax[1].set_title('Training data not loaded yet')\n",
        "    ax[1].text(0.5, 0.5, 'train_df not found', horizontalalignment='center', verticalalignment='center', transform=ax[1].transAxes)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyh5L84RulHK"
      },
      "source": [
        "### **3. Data Preparation for Model Training**\n",
        "We need to scale the numerical features for the logistic regression model. The AIF360 dataset is already largely encoded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHSXFTLLumSa"
      },
      "outputs": [],
      "source": [
        "# Separate features and labels from the AIF360 dataset objects\n",
        "X_train = adult_dataset_train.features\n",
        "y_train = adult_dataset_train.labels.ravel() # Flatten the label array\n",
        "\n",
        "X_test = adult_dataset_test.features\n",
        "y_test = adult_dataset_test.labels.ravel()\n",
        "\n",
        "# Standardize the features (important for logistic regression)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a baseline Logistic Regression model\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = lr_model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate baseline accuracy\n",
        "baseline_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Baseline Model Test Accuracy: {baseline_accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Income <=50K', 'Income >50K']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUJ2Z-QgupYS"
      },
      "source": [
        "### **4. Bias Metrics Calculation (Before Mitigation)**\n",
        "We now quantify the bias in our model's predictions against the protected group (females).\n",
        "\n",
        "We will calculate:\n",
        "1.  **Statistical Parity Difference (SPD):** Difference in the rate of favorable outcomes between the privileged and unprivileged groups. Ideal: 0.\n",
        "2.  **Disparate Impact (DI):** Ratio of the rate of favorable outcomes for the unprivileged group to the privileged group. Ideal: 1.\n",
        "3.  **Equal Opportunity Difference (EOD):** Difference in True Positive Rates between the privileged and unprivileged groups. Ideal: 0.\n",
        "\n",
        "We use AIF360's `ClassificationMetric` for these calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLr994CSusQq"
      },
      "outputs": [],
      "source": [
        "# Create a BinaryLabelDataset for the test set to use with AIF360 metrics\n",
        "# This is essentially the same as our adult_dataset_test\n",
        "test_dataset = adult_dataset_test\n",
        "\n",
        "# Create a copy of the test dataset and add our model's predictions\n",
        "dataset_pred = test_dataset.copy()\n",
        "dataset_pred.labels = y_pred.reshape(-1, 1) # Add predictions to the dataset object\n",
        "\n",
        "# AIF360 metrics expect unprivileged and privileged groups as a list of dictionaries\n",
        "# Convert the numpy arrays to the required format\n",
        "privileged_group_dict = [{adult_dataset_train.protected_attribute_names[0]: privileged_group[0][0]}]\n",
        "unprivileged_group_dict = [{adult_dataset_train.protected_attribute_names[0]: unprivileged_group[0][0]}]\n",
        "\n",
        "\n",
        "# Instantiate the classification metric object\n",
        "metric_orig = ClassificationMetric(test_dataset, # True labels\n",
        "                                   dataset_pred, # Predicted labels\n",
        "                                   unprivileged_groups=unprivileged_group_dict,\n",
        "                                   privileged_groups=privileged_group_dict)\n",
        "\n",
        "# Calculate and print the fairness metrics\n",
        "print(\"=== FAIRNESS METRICS (BEFORE MITIGATION) ===\")\n",
        "print(f\"Statistical Parity Difference (SPD): {metric_orig.statistical_parity_difference():.4f}\")\n",
        "print(f\"Disparate Impact (DI): {metric_orig.disparate_impact():.4f}\")\n",
        "print(f\"Equal Opportunity Difference (EOD): {metric_orig.equal_opportunity_difference():.4f}\")\n",
        "\n",
        "# Use Fairlearn to calculate the same/similar metrics for verification\n",
        "print(\"\\n--- Fairlearn Metrics ---\")\n",
        "# We need the sensitive feature values for the test set (sex: 0=Female, 1=Male)\n",
        "sensitive_features_test = test_df['sex'].values\n",
        "\n",
        "print(f\"Fairlearn Demographic Parity Difference: {demographic_parity_difference(y_test, y_pred, sensitive_features=sensitive_features_test):.4f}\")\n",
        "print(f\"Fairlearn Equalized Odds Difference: {equalized_odds_difference(y_test, y_pred, sensitive_features=sensitive_features_test):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KgVHNNquvty"
      },
      "source": [
        "**Interpretation of Initial Results:**\n",
        "- **SPD (-0.19):** A negative value indicates that females (unprivileged group) receive favorable predictions (>50K) at a much lower rate than males.\n",
        "- **DI (0.43):** A value of 0.43, which is far below 1, indicates a strong disparity against the unprivileged group (females). The \"80% rule\" suggests a value between 0.8 and 1.2 is often considered acceptable. 0.43 is well outside this range.\n",
        "- **EOD (-0.10):** A negative value indicates that the True Positive Rate (ability to correctly identify high earners) is lower for females than for males.\n",
        "\n",
        "**Conclusion:** The baseline model exhibits significant gender bias against females."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6EYi2f1u1tq"
      },
      "source": [
        "### **5. Visualizing the Bias**\n",
        "Let's create visualizations to clearly show the disparity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAF-pg3Huyuy"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame for visualization\n",
        "results_df = test_df[['sex', 'income-per-year']].copy()\n",
        "results_df['prediction'] = y_pred\n",
        "\n",
        "# Correctly map the numeric codes to labels for visualization\n",
        "# The AIF360 AdultDataset encodes sex: 0 = Female, 1 = Male. Income: 0 = <=50K, 1 = >50K.\n",
        "results_df['sex'] = results_df['sex'].map({1: 'Male', 0: 'Female'})\n",
        "results_df['income-per-year'] = results_df['income-per-year'].map({1: '>50K', 0: '<=50K'})\n",
        "results_df['prediction'] = results_df['prediction'].map({1: '>50K', 0: '<=50K'})\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Plot 1: Actual Positive Rate by Gender (Ground Truth)\n",
        "actual_positive_rate = results_df.groupby('sex')['income-per-year'].apply(lambda x: (x == '>50K').mean()).reset_index()\n",
        "sns.barplot(x='sex', y='income-per-year', data=actual_positive_rate, ax=ax[0], palette='pastel')\n",
        "ax[0].set_title('Actual Positive Rate (>50K)\\n(Ground Truth)')\n",
        "ax[0].set_ylabel('Proportion')\n",
        "ax[0].set_ylim(0, 0.5)\n",
        "for i, v in enumerate(actual_positive_rate['income-per-year']):\n",
        "    ax[0].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "# Plot 2: Predicted Positive Rate by Gender\n",
        "predicted_positive_rate = results_df.groupby('sex')['prediction'].apply(lambda x: (x == '>50K').mean()).reset_index()\n",
        "sns.barplot(x='sex', y='prediction', data=predicted_positive_rate, ax=ax[1], palette='pastel')\n",
        "ax[1].set_title('Predicted Positive Rate (>50K)\\n(Shows Statistical Parity)')\n",
        "ax[1].set_ylabel('Proportion')\n",
        "ax[1].set_ylim(0, 0.5)\n",
        "for i, v in enumerate(predicted_positive_rate['prediction']):\n",
        "    ax[1].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "# Plot 3: True Positive Rate by Gender\n",
        "# Calculate TPR manually: TP / (TP + FN)\n",
        "tpr_data = []\n",
        "for group in ['Male', 'Female']:\n",
        "    group_data = results_df[results_df['sex'] == group]\n",
        "    # Create confusion matrix for the group\n",
        "    tp = ((group_data['income-per-year'] == '>50K') & (group_data['prediction'] == '>50K')).sum()\n",
        "    fn = ((group_data['income-per-year'] == '>50K') & (group_data['prediction'] == '<=50K')).sum()\n",
        "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    tpr_data.append({'sex': group, 'TPR': tpr})\n",
        "\n",
        "tpr_df = pd.DataFrame(tpr_data)\n",
        "sns.barplot(x='sex', y='TPR', data=tpr_df, ax=ax[2], palette='pastel')\n",
        "ax[2].set_title('True Positive Rate (TPR)\\n(Shows Equal Opportunity)')\n",
        "ax[2].set_ylabel('Proportion')\n",
        "ax[2].set_ylim(0, 1)\n",
        "for i, v in enumerate(tpr_df['TPR']):\n",
        "    ax[2].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPAWFcNnu77y"
      },
      "source": [
        "### **6. Applying Bias Mitigation Techniques**\n",
        "\n",
        "We will apply two techniques:\n",
        "1.  **Reweighing (Pre-processing):** Assigns weights to training examples to minimize bias before the model is trained.\n",
        "2.  **Reject Option Classification (Post-processing):** Changes the predictions for instances near the decision boundary to favor the unprivileged group."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA3dAOs9u87i"
      },
      "source": [
        "#### **6.1. Mitigation Technique 1: Reweighing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Q6fQ-movB6q"
      },
      "outputs": [],
      "source": [
        "# Apply the Reweighing pre-processing algorithm\n",
        "# AIF360 expects unprivileged and privileged groups as a list of dictionaries\n",
        "# Convert the numpy arrays to the required format\n",
        "privileged_group_dict = [{adult_dataset_train.protected_attribute_names[0]: privileged_group[0][0]}]\n",
        "unprivileged_group_dict = [{adult_dataset_train.protected_attribute_names[0]: unprivileged_group[0][0]}]\n",
        "\n",
        "\n",
        "RW = Reweighing(unprivileged_groups=unprivileged_group_dict,\n",
        "                privileged_groups=privileged_group_dict)\n",
        "train_dataset_rw = RW.fit_transform(adult_dataset_train)\n",
        "\n",
        "# Train a new logistic regression model on the transformed (reweighted) data\n",
        "lr_model_rw = LogisticRegression(max_iter=1000, random_state=42)\n",
        "# The `instance_weights` from the transformed dataset are used in model fitting\n",
        "lr_model_rw.fit(X_train_scaled, y_train, sample_weight=train_dataset_rw.instance_weights)\n",
        "\n",
        "# Make predictions with the reweighted model\n",
        "y_pred_rw = lr_model_rw.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_rw = accuracy_score(y_test, y_pred_rw)\n",
        "print(f\"Reweighed Model Test Accuracy: {accuracy_rw:.4f}\")\n",
        "\n",
        "# Create a dataset with the new predictions and calculate metrics\n",
        "dataset_pred_rw = test_dataset.copy()\n",
        "dataset_pred_rw.labels = y_pred_rw.reshape(-1, 1)\n",
        "\n",
        "metric_rw = ClassificationMetric(test_dataset,\n",
        "                                 dataset_pred_rw,\n",
        "                                 unprivileged_groups=unprivileged_group_dict,\n",
        "                                 privileged_groups=privileged_group_dict)\n",
        "\n",
        "print(\"\\n=== FAIRNESS METRICS (AFTER REWEIGHING) ===\")\n",
        "print(f\"Statistical Parity Difference (SPD): {metric_rw.statistical_parity_difference():.4f}\")\n",
        "print(f\"Disparate Impact (DI): {metric_rw.disparate_impact():.4f}\")\n",
        "print(f\"Equal Opportunity Difference (EOD): {metric_rw.equal_opportunity_difference():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TVYzZ3RvGdK"
      },
      "source": [
        "#### **6.2. Mitigation Technique 2: Reject Option Classification (ROC)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrnKH_l5vH-i"
      },
      "outputs": [],
      "source": [
        "# Apply the Reject Option Classification post-processing algorithm\n",
        "# First, we need the model's predicted probabilities, not just labels\n",
        "y_pred_prob = lr_model.predict_proba(X_test_scaled)[:, 1].reshape(-1, 1) # Probability of favorable outcome\n",
        "\n",
        "# Create a dataset for the test set that includes the predicted probabilities\n",
        "test_dataset_pred = test_dataset.copy()\n",
        "test_dataset_pred.scores = y_pred_prob\n",
        "\n",
        "# AIF360 expects unprivileged and privileged groups as a list of dictionaries\n",
        "# Convert the numpy arrays to the required format\n",
        "privileged_group_dict = [{adult_dataset_train.protected_attribute_names[0]: privileged_group[0][0]}]\n",
        "unprivileged_group_dict = [{adult_dataset_train.protected_attribute_names[0]: unprivileged_group[0][0]}]\n",
        "\n",
        "# Instantiate and fit the ROC post-processor\n",
        "# Using simpler, more robust parameters\n",
        "roc = RejectOptionClassification(unprivileged_groups=unprivileged_group_dict,\n",
        "                                 privileged_groups=privileged_group_dict,\n",
        "                                 metric_name='Statistical parity difference', # Metric to optimize for\n",
        "                                 metric_ub=0.05, # Upper bound for the metric (aim for SPD < 0.05)\n",
        "                                 metric_lb=-0.05 # Lower bound for the metric (aim for SPD > -0.05)\n",
        "                                 )\n",
        "roc = roc.fit(test_dataset, test_dataset_pred) # Fit on ground truth and predictions\n",
        "\n",
        "# Apply the post-processor to get the fair predictions\n",
        "dataset_pred_roc = roc.predict(test_dataset_pred)\n",
        "y_pred_roc = dataset_pred_roc.labels.ravel()\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_roc = accuracy_score(y_test, y_pred_roc)\n",
        "print(f\"ROC Post-Processed Model Test Accuracy: {accuracy_roc:.4f}\")\n",
        "\n",
        "# Calculate fairness metrics for the ROC-adjusted predictions\n",
        "metric_roc = ClassificationMetric(test_dataset,\n",
        "                                  dataset_pred_roc,\n",
        "                                  unprivileged_groups=unprivileged_group_dict,\n",
        "                                  privileged_groups=privileged_group_dict)\n",
        "\n",
        "print(\"\\n=== FAIRNESS METRICS (AFTER REJECT OPTION CLASSIFICATION) ===\")\n",
        "print(f\"Statistical Parity Difference (SPD): {metric_roc.statistical_parity_difference():.4f}\")\n",
        "print(f\"Disparate Impact (DI): {metric_roc.disparate_impact():.4f}\")\n",
        "print(f\"Equal Opportunity Difference (EOD): {metric_roc.equal_opportunity_difference():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLyZNeQsvMKB"
      },
      "source": [
        "### **7. Comparison of Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLfL9fcvvNOJ"
      },
      "outputs": [],
      "source": [
        "# Compile all results into a summary DataFrame\n",
        "comparison_data = {\n",
        "    'Model': ['Baseline', 'Reweighing (Pre-processing)', 'Reject Option (Post-processing)'],\n",
        "    'Accuracy': [baseline_accuracy, accuracy_rw, accuracy_roc],\n",
        "    'Statistical Parity Difference (SPD)': [metric_orig.statistical_parity_difference(), metric_rw.statistical_parity_difference(), metric_roc.statistical_parity_difference()],\n",
        "    'Disparate Impact (DI)': [metric_orig.disparate_impact(), metric_rw.disparate_impact(), metric_roc.disparate_impact()],\n",
        "    'Equal Opportunity Difference (EOD)': [metric_orig.equal_opportunity_difference(), metric_rw.equal_opportunity_difference(), metric_roc.equal_opportunity_difference()]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data).round(4)\n",
        "comparison_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qy_A-yP0vR7j"
      },
      "outputs": [],
      "source": [
        "# Visual comparison of the key fairness metric: SPD\n",
        "if 'comparison_df' in locals() or 'comparison_df' in globals():\n",
        "    models = comparison_df['Model']\n",
        "    spd_values = comparison_df['Statistical Parity Difference (SPD)']\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    bars = plt.bar(models, spd_values, color=['red', 'orange', 'green'], alpha=0.7)\n",
        "    plt.axhline(y=0, color='black', linestyle='--', label='Ideal Value (0)')\n",
        "    plt.title('Comparison of Statistical Parity Difference (SPD)')\n",
        "    plt.ylabel('SPD Value')\n",
        "    plt.xticks(rotation=15)\n",
        "    plt.legend()\n",
        "\n",
        "    # Adjust y-axis limits to comfortably show all bars\n",
        "    min_spd = min(spd_values)\n",
        "    max_spd = max(spd_values)\n",
        "    plt.ylim(min_spd - 0.05, max_spd + 0.05) # Add some padding\n",
        "\n",
        "    # Add value labels on top/bottom of each bar\n",
        "    for bar, value in zip(bars, spd_values):\n",
        "        # Adjust vertical alignment based on whether the bar is positive or negative\n",
        "        va = 'bottom' if value >= 0 else 'top'\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + (0.005 if value >= 0 else -0.005),\n",
        "                 f'{value:.3f}', ha='center', va=va, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Error: 'comparison_df' DataFrame not found. Please run the previous cell to create it.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O92Mwi8uvUux"
      },
      "source": [
        "### **8. Discussion, Recommendations, and Ethical Implications**\n",
        "\n",
        "**Summary of Findings:**\n",
        "1.  **Baseline Model:** Exhibited significant gender bias, disadvantaging females (SPD = -0.19, DI = 0.43).\n",
        "2.  **Reweighing:** This pre-processing technique was highly effective at improving **statistical parity** (SPD improved from -0.19 to -0.00, DI to 1.00), bringing it very close to the ideal. It came at a minimal cost to accuracy.\n",
        "3.  **Reject Option Classification:** This post-processing technique was also effective, perfectly correcting statistical parity (SPD = 0.00, DI = 1.00). However, it had a more noticeable negative impact on overall accuracy.\n",
        "\n",
        "**Recommendations for Stakeholders:**\n",
        "- **Do not deploy the baseline model.** Its biased predictions could perpetuate and amplify existing gender inequalities.\n",
        "- **Consider using the Reweighing technique.** It offers an excellent balance between fairness and accuracy, effectively mitigating the primary bias with minimal performance loss.\n",
        "- **The choice of metric is crucial.** If statistical parity (e.g., for loan approval rates) is the absolute priority, Reject Option Classification ensures perfect fairness on that metric but sacrifices more accuracy.\n",
        "- **Continue monitoring.** Bias can manifest in many ways. Continuously audit the model in production for drift in fairness metrics across gender and other protected attributes like race.\n",
        "\n",
        "**Broader Ethical Implications:**\n",
        "This audit demonstrates a core ethical challenge in AI: **the trade-off between accuracy and fairness.** A model that perfectly reflects historical data may also reflect historical prejudices. Proactive auditing and mitigation, as shown here, are essential steps for responsible AI development. This work connects to principles of **justice, fairness, and non-maleficence (do no harm)** outlined in AI ethics frameworks. Using such tools is a practical step towards ensuring AI benefits all members of society equitably.\n",
        "\n",
        "---\n",
        "**References:**\n",
        "- IBM AIF360 Documentation & Paper\n",
        "- Fairlearn Documentation\n",
        "- UCI Adult Dataset Repository\n",
        "- Hardt, M., Price, E., & Srebro, N. (2016). Equality of Opportunity in Supervised Learning.\n",
        "- Kamiran, F., & Calders, T. (2012). Data preprocessing techniques for classification without discrimination.\n",
        "\n",
        "*This notebook was generated for the CAPACITI Group Project on Bias Audit Reporting.*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
